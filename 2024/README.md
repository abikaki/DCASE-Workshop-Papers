# DCASE-2024-Workshop Papers
<a href="https://dcase.community/workshop2024/"><img src="dcase2024-logo.png" width="800"/></a>

The 9<sup>th</sup> Workshop on Detection and Classification of Acoustic Scenes and Events, DCASE 2024, took place in Tokyo, Japan on 23-25 October 2024.


<i>Nobutaka Ono Noboru Harada, Yohei Kawaguchi, Woon-Seng Gan, Keisuke Imoto, Tatsuya Komatsu, Qiuqiang Kong, Irene Martin Morato (eds.), [Proceedings of the 9th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2024)](https://zenodo.org/records/13871309/files/DCASE2024.pdf?download=1). Tokyo, Japan, 2024.</i>


## Papers
| **ID** | **Title** | **Repo** | **PDF** | **Abstract**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| ------ | ----------| :------: |:-------:|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1 | IMAD-DS: A Dataset for Industrial Multi-Sensor Anomaly Detection Under Domain Shift Conditions | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Albertini_45.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Industrial anomaly detection (AD) plays a critical role in maintaining the safety, efficiency and productivity of modern manufacturing and production processes. Despite the widespread adoption of IoT sensor boards in industry, there is still a lack of comprehensive multi-sensor and multi-rate datasets for AD that adequately account for domain shifts, i.e. variations in operational and environmental conditions that significantly affect AD performance. To address this gap, we present the Industrial Multi-sensor Anomaly Detection under Domain Shift Conditions (IMAD-DS) dataset. The IMAD-DS dataset comprises multi-sensor data from two scaled industrial machines: a robotic arm and a brushless motor, collected under different operating conditions to mimic real-world domain shifts, including speed and load changes. We also add different types of background noise to the audio data to simulate different environmental domain shifts. Benchmark testing with an autoencoder model show that AD performance decreases significantly with domain shifts, emphasizing the value of IMAD-DS for the development of robust multi-sensor AD systems.</div></details> |
| 2 | Angular Distance Distribution Loss for Audio Classification | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Almudevar_25.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Classification is a pivotal task in deep learning not only because of its intrinsic importance, but also for providing embeddings with desirable properties in other tasks. To optimize these properties, a wide variety of loss functions have been proposed that attempt to minimize the intra-class distance and maximize the inter-class distance in the embeddings space. In this paper we argue that, in addition to these two, eliminating hierarchies within and among classes are two other desirable properties for classification embeddings. Furthermore, we propose the Angular Distance Distribution (ADD) Loss, which aims to enhance the four previous properties jointly. For this purpose, it imposes conditions on the first and second order statistical moments of the angular distance between embeddings. Finally, we perform experiments showing that our loss function improves all four properties and, consequently, performs better than other loss functions in audio classification tasks.</div></details> |
| 3 | Heterogeneous Sound Classification with the Broad Sound Taxonomy and Dataset | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Anastasopoulou_39.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Automatic sound classification has a wide range of applications in machine listening, enabling context-aware sound processing and understanding. This paper explores methodologies for automatically classifying heterogeneous sounds characterized by high intraclass variability. Our study evaluates the classification task using the Broad Sound Taxonomy, a two-level taxonomy comprising 28 classes designed to cover a heterogeneous range of sounds with semantic distinctions tailored for practical user applications. We construct a dataset through manual annotation to ensure accuracy, diverse representation within each class and relevance in real-world scenarios. We compare a variety of both traditional and modern machine learning approaches to establish a baseline for the task of heterogeneous sound classification. We investigate the role of input features, specifically examining how acoustically derived sound representations compare to embeddings extracted with pre-trained deep neural networks that capture both acoustic and semantic information about sounds. Experimental results illustrate that audio embeddings encoding acoustic and semantic information achieve higher accuracy in the classification task. After careful analysis of classification errors, we identify some underlying reasons for failure and propose actions to mitigate them. The paper highlights the need for deeper exploration of all stages of classification, understanding the data and adopting methodologies capable of effectively handling data complexity and generalizing in real-world sound environments.</div></details> |
| 4 | Learning Multi-Target TDOA Features for Sound Event Localization and Detection | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Berg_46.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Sound event localization and detection (SELD) systems using audio recordings from a microphone array rely on spatial cues for determining the location of sound events. As a consequence, the localization performance of such systems is to a large extent determined by the quality of the audio features that are used as inputs to the system. We propose a new feature, based on neural generalized cross-correlations with phase-transform (NGCC-PHAT), that learns audio representations suitable for localization. Using permutation invariant training for the time-difference of arrival (TDOA) estimation problem enables NGCC-PHAT to learn TDOA features for multiple overlapping sound events. These features can be used as a drop-in replacement for GCC-PHAT inputs to a SELD-network. We test our method on the STARSS23 dataset and demonstrate improved localization performance compared to using standard GCC-PHAT or SALSA-Lite input features.</div></details> |
| 5 | Leveraging Self-Supervised Audio Representations for Data-Efficient Acoustic Scene Classification | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Cai_35.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Acoustic scene classification (ASC) predominantly relies on supervised approaches. However, acquiring labeled data for training ASC models is often costly and time-consuming. Recently, self-supervised learning (SSL) has emerged as a powerful method for extracting features from unlabeled audio data, benefiting many downstream audio tasks. This paper proposes a data-efficient and low-complexity ASC system by leveraging self-supervised audio representations extracted from general-purpose audio datasets. We introduce BEATs, an audio SSL pre-trained model, to extract the general representations from AudioSet. Through extensive experiments, it has been demonstrated that the self-supervised audio representations can help to achieve high ASC accuracy with limited labeled fine-tuning data. Furthermore, we find that ensembling the SSL models fine-tuned with different strategies contributes to a further performance improvement. To meet low-complexity requirements, we use knowledge distillation to transfer the self-supervised knowledge from large teacher models to an efficient student model. The experimental results suggest that the self-supervised teachers effectively improve the classification accuracy of the student model. Our best-performing system obtains an average accuracy of 56.7%.</div></details> |
| 6 | Language-Queried Audio Source Separation Enhanced by Expanded Language-Audio Contrastive Loss | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Chung_23.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Audio sources recorded for specific purposes often contain extraneous sounds that deviate from the intended goal. Re-recording to achieve the desired result is expensive. However, separating the target source from the original audio source based on natural language queries would be much more efficient. However, audio source separation with natural language queries is a complex task. To address this, the DCASE 2024 Challenge Task 9 proposed language-queried audio source separation (LASS). This paper aims to tackle LASS by proposing an extended language-audio contrastive learning approach. To align the separated output audio with the target text and target audio, we first designed audio-to-text contrastive loss and audio-to-audio contrastive loss, respectively. By leveraging the characteristics of contrastive learning, we combined these two losses into an extended audio-to-multi contrastive loss. Our model, trained with this loss, improves the signal-to-distortion ratio (SDR) by more than 30% compared to the baseline provided by the challenge.</div></details> |
| 7 | DCASE 2024 Task 4: Sound Event Detection with Heterogeneous Data and Missing Labels | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Cornell_30.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">The Detection and Classification of Acoustic Scenes and Events Challenge Task 4 aims to advance sound event detection (SED) systems by leveraging training data with different supervision uncertainty. Participants are challenged in exploring how to best use training data from different domains and with varying annotation granularity (strong/weak temporal resolution, soft/hard labels), to obtain a robust SED system that can generalize across different scenarios. Crucially, annotation across available training datasets can be inconsistent and hence sound events of one dataset may be present but not annotated in an other one. As such, systems have to cope with potentially missing target labels during training. Moreover, as an additional novelty, systems are also evaluated on labels with different granularity in order to assess their robustness for different applications. To lower the entry barrier for participants, we developed an updated baseline system with several caveats to address these aforementioned problems. Results with our baseline system indicate that this research direction is promising and it is possible to obtain a stronger SED system by using diverse domain training data with missing labels compared to training a SED system for each domain separately.</div></details> |
| 8 | Frequency Tracking Features for Data-Efficient Deep Siren Identification | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Damiano_41.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">The identification of siren sounds in urban soundscapes is a crucial safety aspect for smart vehicles and has been widely addressed by means of neural networks that ensure robustness to both the diversity of siren signals and the strong and unstructured background noise characterizing traffic. Convolutional neural networks analyzing spectrogram features of incoming signals achieve state-of-the-art performance when enough training data capturing the diversity of the target acoustic scenes is available. In practice, data is usually limited and algorithms should be robust to adapt to unseen acoustic conditions without requiring extensive datasets for re-training. In this work, given the harmonic nature of siren signals, characterized by a periodically evolving fundamental frequency, we propose a low-complexity feature extraction method based on frequency tracking using a single-parameter adaptive notch filter. The features are then used to design a small-scale convolutional network suitable for training with limited data. The evaluation results indicate that the proposed model consistently outperforms the traditional spectrogram-based model when limited training data is available, achieves better cross-domain generalization and has a smaller size.</div></details> |
| 9 | Baseline Models and Evaluation of Sound Event Localization and Detection with Distance Estimation in DCASE2024 Challenge | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Diaz-Guerra_53.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">This technical report presents the objectives, evaluation, and baseline changes for Task 3, Sound Event Localization and Detection (SELD), of the DCASE2024 Challenge. While the development and evaluation dataset, STARSS23, and the division of the task into two tracks, audio-only and audiovisual (AV), remain the same, this year introduces source distance estimation (SDE) along with detection and direction-of-arrival (DOA) estimation of target sound events. Changes in task evaluation metrics and the design and training of the baseline models due to this new SDE subtask are detailed in the report and compared with the previous iteration of the challenge. Further baseline improvements regarding the integration of video information are also presented. Overall, the design of highly effective SELD models evaluated in real scenes with a limited volume of unbalanced training data has proven challenging. The introduction of SDE makes the task even more demanding, as evidenced by the low spatially-thresholded detection scores for both audio-only and AV baselines. While distance estimation error results seem promising, this comes at the expense of lower detection and DOA estimation scores compared to the previous year’s baseline models without SDE. Based on the current AV model design, video integration does not bring apparent estimation benefits compared to using only audio input, indicating that more research is required into more effective fusion strategies, model architectures, data augmentation and simulation methods, or training strategies.</div></details> |
| 10 | From Computation to Consumption: Exploring the Compute-Energy Link for Training and Testing Neural Networks for Sed Systems | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Douwes_4.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">The massive use of machine learning models, particularly neural networks, has raised serious concerns about their environmental impact. Indeed, over the last few years we have seen an explosion in the computing costs associated with training and deploying these systems. It is, therefore, crucial to understand their energy requirements in order to better integrate them into the evaluation of models, which has so far focused mainly on performance. In this paper, we study several neural network architectures that are key components of sound event detection systems, using an audio tagging task as an example. We measure the energy consumption for training and testing small to large architectures and establish complex relationships between the energy consumption, the number of floating-point operations, the number of parameters, and the GPU/memory utilization.</div></details> |
| 11 | Improving Query-By-Vocal Imitation with Contrastive Learning and Audio Pretraining | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Greif_36.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Query-by-Vocal Imitation (QBV) is about searching audio files within databases using vocal imitations created by the user’s voice. Since most humans can effectively communicate sound concepts through voice, QBV offers the more intuitive and convenient approach compared to text-based search. To fully leverage QBV, developing robust audio feature representations for both the vocal imitation and the original sound is crucial. In this paper, we present a new system for QBV that utilizes the feature extraction capabilities of Convolutional Neural Networks pre-trained with large-scale general-purpose audio datasets. We integrate these pre-trained models into a dual encoder architecture and fine-tune them end-to-end using contrastive learning. A distinctive aspect of our proposed method is the fine-tuning strategy of pre-trained models using an adapted NT-Xent loss for contrastive learning, creating a shared embedding space for reference recordings and vocal imitations. The proposed system significantly enhances audio retrieval performance, establishing a new state of the art on both coarse- and fine-grained QBV tasks.</div></details> |
| 12 | Does Paid Crowdsourcing Still Pay Off? Sifting Through Annotator Noise in Crowdsourced Audio Labels | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Harju_18.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Paid crowdsourcing has emerged as a popular method for annotating diverse data types such as images, text, and audio. However, the amount of carelessly working annotators has increased as platforms have become more popular, leading to an influx of spam workers that answer at random, which renders the platforms unusable. This paper documents our attempt to annotate the DESED dataset using Amazon’s Mechanical Turk, and failing to obtain any useful data after two attempts. Our observations reveal that while the number of workers performing the tasks has increased since 2021, the quality of obtained labels has declined considerably. After successful trials for annotating audio data in 2021 and 2022, in 2024 the same user interface annotation setup predominantly attracted spammers. Given the consistent task setup and similarity to previous attempts, it remains unclear whether the workers are inherently subpar or if they are intentionally exploiting the platform. The bottom line is that despite spending a considerable amount of money on it, we obtained no usable data.</div></details> |
| 13 | EnCLAP++: Analyzing the EnCLAP Framework for Optimizing Automated Audio Captioning Performance | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Kim_70.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">In this work, we aim to analyze and optimize the EnCLAP framework, a state-of-the-art model in automated audio captioning. We investigate the impact of modifying the acoustic encoder components, explore pretraining with different dataset scales, and study the effectiveness of a reranking scheme. Through extensive experimentation and quantitative analysis of generated captions, we develop EnCLAP++, an enhanced version that significantly surpasses the original.</div></details> |
| 14 | CLAP4SED: Training-Free Multimodal Few-Shot Retrieval for Real-Time Sound Event Detection on Embedded Devices | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Lin_14.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Implementing real-time sound event detection (SED) on embedded devices poses significant challenges, primarily related to generalizability and complexity. Existing SED models are predominantly suited for closed-form recognition, making adaptation to new or unseen sound classes difficult. While recent advancements in audio foundation models (AFM) such as CLAP offer potential for open-form sound event classification, they often come with substantial model complexity, rendering them impractical to deploy on embedded devices for real-time tracking. In this study, we introduce the CLAP4SED framework, a training-free, real-time SED solution derived from CLAP that can be flexibly deployed across various open-ended scenarios on embedded devices. Our experimental results conducted on three publicly available datasets demonstrating the competitive SED accuracy with less than 100ms latency under Ambarella CV22 camera chip setup.</div></details> |
| 15 | Guided Captioning of Audio | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Martin-Morato_26.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">This work introduces a guided captioning system that aims to produce captions focused on different audio content, depending on a guiding text. We show that using keywords guidance results in more diverse captions, even though the usual captioning metrics do not reflect this. We design a system that can be trained using keywords automatically extracted from reference annotations, and which is provided with one keyword at test time. When trained with 5 keywords, the produced captions contain the exact guidance keyword 70% of the time, and results in over 3600 unique sentences for Clotho dataset. In contrast, a baseline without any keywords produces 700 unique captions on the same test set.</div></details> |
| 16 | Audio Captioning in Finnish and English with Task-Dependent Output | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Martin-Morato_27.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Describing audio content is a complex task for an annotator; the resulting caption depends on the annotator’s language, culture and expertise. In addition, physiological factors like vision impairment may affect on how the sound is perceived and interpreted. In this work, we explore bilingual audio captioning in Finnish and English. In connection with this study, we release the SiVi-CAFE dataset, a small-size dataset of Sighted and Visually-impaired Captions for Audio in Finnish and English, with a collection of parallel annotations for the same clips. We analyze briefly the differences between captions produced by sighted and visually-impaired annotators, and train a system to produce captions in both languages that also mimics the style of different annotator groups. Obtaining a CIDEr score of 34.75% and 28.75% on the English and Finnish datasets, respectively. Furthermore, the system is able to perform a tagging task, obtaining F-score of 79.73%.</div></details> |
| 17 | Acoustic Scene Classification Across Multiple Devices Through Incremental Learning of Device-Specific Domains | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Mulimani_17.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">In this paper, we propose using a domain-incremental learning approach for coping with different devices in acoustic scene classification. While the typical way to handle mismatched training data is through domain adaptation or specific regularization techniques, incremental learning offers a different approach. With this technique, it is possible to learn the characteristics of new devices on-the-go, adding to a previously trained model. This also means that new device data can be introduced at any time, without a need to retrain the original model. In terms of incremental learning, we propose a combination of domain-specific Low-Rank Adaptation (LoRA) parameters and running statistics of Batch Normalization (BN) layers. LoRA adds low-rank decomposition matrices to a convolutional layer with a few trainable parameters for each new device, while domain-specific BN is used to boost performance. Experiments are conducted on the TAU Urban Acoustic Scenes 2020 Mobile development dataset, containing 9 different devices; we train the system using the 40h of data available for the main device, and incrementally learn the domains of the other 8 devices based on 3h of data available for each. We show that the proposed approach outperforms other fine-tuning-based methods, and is outperformed only by joint learning with all data from all devices.</div></details> |
| 18 | Pre-Trained Models, Datasets, Data Augmentation for Language-Based Audio Retrieval | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Munakata_60.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">We investigate the impact of pre-trained models, datasets, and data augmentation on language-based audio retrieval. Despite the high interest in cross-modal retrieval and the introduction of various datasets, powerful encoders, and data augmentation techniques, it remains unclear which approaches are most effective for language-based audio retrieval. We focus on which should be selected to build a retrieval model. First, we investigate the performance gain by four audio encoders, PaSST, CAV-MAE, BEATs, and VAST, and three text encoders BERT, RoBERTa, and T5. Second, we prepare massive datasets of over 670k audio-text pairs including ClothoV2, AudioCaps, WavCaps, MACS, and Auto-ACD. Third, we investigate the combination of data augmentation methods to enhance the retrieval performance including mixup-contrast and text token masking. In addition, we also explore inference time augmentation by paraphrasing textual queries using Chat-GPT to achieve robust retrieval performance. Our final results achieve 39.79 points with a single model and 42.22 points with the ensemble models in the mean average precision among the top 10 results on the evaluation split of ClothoV2.</div></details> |
| 19 | Data-Efficient Acoustic Scene Classification with Pre-Training, Bayesian Ensemble Averaging, and Extensive Augmentations | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Nadrchal_47.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">The task of Acoustic Scene Classification (ASC) is to categorize short audio recordings into predefined scene classes. The DCASE community hosts an annual competition on ASC with a special focus on real-world problems such as recording device mismatches, low-complexity constraints, and limited labelled data availability. Solutions like Knowledge Distillation (KD) and task-specific data augmentations have proven effective in tackling these challenges, as demonstrated by their successful application in top-ranked systems. This paper contributes to the research on the real-world applicability of ASC systems by analyzing the effect of AudioSet pre-training on downstream training sets of different sizes. We study the impact of extensive data augmentation techniques, including Freq-MixStyle, device impulse response augmentation, FilterAugment, frequency masking, and time rolling on different training set sizes. Furthermore, the effectiveness of Bayesian Ensemble Averaging over traditional mean ensembling in KD is investigated. The results demonstrate that the proposed methods improve the performance over the DCASE baseline system substantially, with a particularly large gain on the smallest training set, lifting the accuracy by more than 7 percentage points on the development-test split.</div></details> |
| 20 | Self Training and Ensembling Frequency Dependent Networks with Coarse Prediction Pooling and Sound Event Bounding Boxes | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Nam_16.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">To tackle sound event detection (SED), we propose frequency dependent networks (FreDNets), which heavily leverage frequency-dependent methods. We apply frequency warping and FilterAugment, which are frequency-dependent data augmentation methods. The model architecture consists of 3 branches: audio teacher-student transformer (ATST) branch, BEATs branch and CNN branch including either partial dilated frequency dynamic convolution (PDFD conv) or squeeze-and-Excitation (SE) with time-frame frequency-wise SE (tfwSE). To train MAESTRO labels with coarse temporal resolution, we applied max pooling on prediction for the MAESTRO dataset. Using best ensemble model, we applied self training to obtain pseudo label from DESED weak set, unlabeled set and AudioSet. AudioSet pseudo labels, filtered to focus on high-confidence labels, are used to train on DESED dataset only. We used change-detection-based sound event bounding boxes (cSEBBs) as post processing for ensemble models on self training and submission models. The resulting FreDNet was ranked 2nd in DCASE 2024 Challenge Task 4.</div></details> |
| 21 | Improving Domain Generalisation with Diversity-based Sampling | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Napoli_19.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Domain shifts are a major obstacle to the deployment of automated bioacoustic monitoring tools to new recording environments or habitats. Invariance regularisation is one approach for dealing with these shifts, in which the feature distributions of data from different domains are encouraged to match (by minimising some measure of statistical distance). However, in a deep learning setup, the statistical distance is only computed over small minibatches of data at a time. Inevitably, small samples have poor representation of their underlying distributions, resulting in extremely noisy distance estimates. In this paper, we propose that promoting wider distribution coverage, by inducing diversity in each sampled minibatch, would improve these estimates, and hence the generalisation power of the trained model. We describe two options for diversity-based data samplers, based on the κ-determinantal point process (κ-DPP) and the κ-means++ algorithm, which can function as drop-in replacements for a standard random sampler. We then test these on a domain shift task based on humpback whale detection, where we find both options improve the performance of two invariance regularisation algorithms, as well as standard training via empirical risk minimisation (ERM).</div></details> |
| 22 | ToyADMOS2#: Yet Another Dataset for the DCASE2024 Challenge Task 2 First-Shot Anomalous Sound Detection | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Niizumi_5.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">First-shot anomalous sound detection (ASD) is a task designed to challenge a system’s applicability to new data based on the needs of real-world application scenarios. This paper describes new ToyADMOS2 data to evaluate the first-shot compliant systems for the DCASE2024 Challenge Task 2, First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring. The new data is designed to differ from the previous in the new machine sounds, including HoveringDrone, HairDryer, ToyCircuit, and ToothBrush, as well as in that each sound has a different background noise. The HairDryer and ToothBrush sounds are also designed as examples of ASD application scenarios in factory pre-shipment inspections, and we confirm their potential in the evaluation. We detail these data and show the baseline performance for reference in future studies.</div></details> |
| 23 | Description and Discussion on DCASE 2024 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Nishida_31.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">We present the task description of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 Challenge Task 2: “First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring”. Continuing from last year’s DCASE 2023 Challenge Task 2, we organize the task as a first-shot problem under domain generalization required settings. The main goal of the first-shot problem is to enable rapid deployment of ASD systems for new kinds of machines without the need for machine-specific hyperparameter tunings. For the DCASE 2024 Challenge Task 2, sounds of new machine types were collected and provided as the evaluation dataset. In addition, attribute information such as the machine operation conditions were concealed for several machine types to simulate situations where such information are unavailable. We received 96 submissions from 27 teams, and an analysis of these submissions has been made in this paper. Several novel approaches, such as new ways of utilizing pre-trained models and pseudo-label classification approaches, have been used to beat the baseline system.</div></details> |
| 24 | A Sound Description: Exploring Prompt Templates and Class Descriptions to Enhance Zero-Shot Audio Classification | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Olvera_67.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Audio-text models trained via contrastive learning offer a practical approach to perform audio classification through natural language prompts, such as “this is a sound of” followed by category names. In this work, we explore alternative prompt templates for zero-shot audio classification, demonstrating the existence of higher-performing options. First, we find that the formatting of the prompts significantly affects performance so that simply prompting the models with properly formatted class labels performs competitively with optimized prompt templates and even prompt ensembling. Moreover, we look into complementing class labels by audio-centric descriptions. By leveraging large language models, we generate textual descriptions that prioritize acoustic features of sound events to disambiguate between classes, without extensive prompt engineering. We show that prompting with class descriptions leads to state-of-the-art results in zero-shot audio classification across major ambient sound datasets. Remarkably, this method requires no additional training and remains fully zero-shot.</div></details> |
| 25 | Estimated Audio–Caption Correspondences Improve Language-Based Audio Retrieval | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Primus_51.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Dual-encoder-based audio retrieval systems are commonly optimized with contrastive learning on a set of matching and mismatching audio–caption pairs. This leads to a shared embedding space in which corresponding items from the two modalities end up close together. Since audio–caption datasets typically only contain matching pairs of recordings and descriptions, it has become common practice to create mismatching pairs by pairing the audio with a caption randomly drawn from the dataset. This is not ideal because the randomly sampled caption could, just by chance, partly or entirely describe the audio recording. However, correspondence information for all possible pairs is costly to annotate and thus typically unavailable; we, therefore, suggest substituting it with estimated correspondences. To this end, we propose a two-staged training procedure in which multiple retrieval models are first trained as usual, i.e., without estimated correspondences. In the second stage, the audio–caption correspondences predicted by these models then serve as prediction targets. We evaluate our method on the ClothoV2 and the AudioCaps benchmark and show that it improves retrieval performance, even in a restricting self-distillation setting where a single model generates and then learns from the estimated correspondences. We further show that our method outperforms the current state of the art by 1.6 pp. mAP@10 on the ClothoV2 benchmark.</div></details> |
| 26 | Synthetic Training Set Generation using Text-To-Audio Models for Environmental Sound Classification | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Ronchini_8.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">In recent years, text-to-audio models have revolutionized the field of automatic audio generation. This paper investigates their application in generating synthetic datasets for training data-driven models. Specifically, this study analyzes the performance of two environmental sound classification systems trained with data generated from text-to-audio models. We considered three scenarios: a) augmenting the training dataset with data generated by text-to-audio models; b) using a mixed training dataset combining real and synthetic text-driven generated data; and c) using a training dataset composed entirely of synthetic audio. In all cases, the performance of the classification models was tested on real data. Results indicate that text-to-audio models are effective for dataset augmentation, with consistent performance when replacing a subset of the recorded dataset. However, the performance of the audio recognition models drops when relying entirely on generated audio.</div></details> |
| 27 | Prediction of Pleasantness and Eventfulness Perceptual Sound Qualities in Urban Soundscapes | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Sagasti_12.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">The acoustic environment induces emotions in human listeners. To describe such emotions, ISO-12913 defines pleasantness and eventfulness as orthogonal properties that characterise urban soundscapes. In this paper, we study different approaches for automatically estimating these two perceptual sound qualities. We emphasize the comparison of three sets of audio features: a first set from the acoustic and psychoacoustic domain, suggested in ISO-12913; a second set of features from the machine listening domain based on traditional signal processing algorithms; and a third set consisting of audio embeddings generated with a pre-trained audio-language deep-learning model. Each feature set is tested on its own and in combination with ground-truth labels about the sound sources present in the recordings to determine if this additional information improves the prediction accuracy. Our findings indicate that the deep-learning representation yields slightly better performance than the other feature sets when predicting pleasantness, but all of them yield similar performance when predicting eventfulness. Nevertheless, deep-learning embeddings present other advantages, such as faster calculation times and greater robustness against changes in sensor calibration, making them more effective for real-time acoustic monitoring. Furthermore, we observe a clear correlation between the sound sources that are present in the urban soundscape and its induced emotions, specially regarding the sensation of pleasantness. Models like the ones proposed in this paper allow for an assessment of the acoustic environment that goes beyond a characterisation solely based on sound pressure level measurements and could be integrated into current acoustic monitoring solutions to enhance the understanding from the perspective of the induced emotions.</div></details> |
| 28 | Data-Efficient Low-Complexity Acoustic Scene Classification in the DCASE 2024 Challenge | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Schmid_15.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">This article describes the Data-Efficient Low-Complexity Acoustic Scene Classification Task in the DCASE 2024 Challenge and the corresponding baseline system. The task setup is a continuation of previous editions (2022 and 2023), which focused on recording device mismatches and low-complexity constraints. This year’s edition introduces an additional real-world problem: participants must develop data-efficient systems for five scenarios, which progressively limit the available training data. The provided baseline system is based on an efficient, factorized CNN architecture constructed from inverted residual blocks and uses Freq-MixStyle to tackle the device mismatch problem. The task received 37 submissions from 17 teams, with the large majority of systems outperforming the baseline. The top-ranked system’s accuracy ranges from 54.3% on the smallest to 61.8% on the largest subset, corresponding to relative improvements of approximately 23% and 9% over the baseline system on the evaluation set.</div></details> |
| 29 | Multi-Iteration Multi-Stage Fine-Tuning of Transformers for Sound Event Detection with Heterogeneous Datasets | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Schmid_28.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">A central problem in building effective sound event detection systems is the lack of high-quality, strongly annotated sound event datasets. For this reason, Task 4 of the DCASE 2024 challenge proposes learning from two heterogeneous datasets, including audio clips labeled with varying annotation granularity and with different sets of possible events. We propose a multi-iteration, multi-stage procedure for fine-tuning Audio Spectrogram Transformers on the joint DESED and MAESTRO Real datasets. The first stage closely matches the baseline system setup and trains a CRNN model while keeping the pre-trained transformer model frozen. In the second stage, both CRNN and transformer are fine-tuned using heavily weighted self-supervised losses. After the second stage, we compute strong pseudo-labels for all audio clips in the training set using an ensemble of fine-tuned transformers. Then, in a second iteration, we repeat the two-stage training process and include a distillation loss based on the pseudo-labels, achieving a new single-model, state-of-the-art performance on the public evaluation set of DESED with a PSDS1 of 0.692. A single model and an ensemble, both based on our proposed training procedure, ranked first in Task 4 of the DCASE Challenge 2024.</div></details> |
| 30 | Representational Learning for an Anomalous Sound Detection System with Source Separation Model | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Shin_68.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">The detection of anomalous sounds in machinery operation presents a significant challenge due to the difficulty in generalizing anomalous acoustic patterns. This task is typically approached as an unsupervised learning or novelty detection problem, given the complexities associated with the acquisition of comprehensive anomalous acoustic data. Conventional methodologies for training anomalous sound detection systems primarily employ auto-encoder architectures or representational learning with auxiliary tasks. However, both approaches have inherent limitations. Auto-encoder structures are constrained to utilizing only the target machine’s operational sounds, while training with auxiliary tasks, although capable of incorporating diverse acoustic inputs, may yield representations that lack correlation with the characteristic acoustic signatures of anomalous conditions. We propose a training method based on the source separation model (CMGAN[1]) that aims to isolate non-target machine sounds from a mixture of target and non-target class acoustic signals. This approach enables the effective utilization of diverse machine sounds and facilitates the training of complex neural network architectures with limited sample sizes. Our experimental results demonstrate that the proposed method yields better performance compared to both conventional auto-encoder training approaches and source separation techniques that focus on isolating target machine signals. Moreover, our experimental results demonstrate that the proposed method exhibits the potential for enhanced representation learning as the quantity of non-target data increases, even while maintaining a constant volume of target class data.</div></details> |
| 31 | Auxiliary Decoder-Based Learning of Sound Event Detection using Multi-Channel Features and Maximum Probability Aggregation | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Son_58.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">This paper proposes a sound event detection (SED) model operating on heterogeneous labeled and/or unlabeled datasets, such as the DESED and MAESTRO datasets. The proposed SED model is based on a frequency dynamic convolution (FDY)–large kernel attention (LKA)-convolutional recurrent neural network (CRNN), and it is trained via mean-teacher-based semi-supervised learning to handle unlabeled data. The FDY–LKA-CRNN model incorporates bidirectional encoder representation from audio transformer (BEATs) embeddings to improve high-level semantic representation. However, the contribution of the BEATs encoder to the performance of the combined SED model is over-emphasized relative to that of the FDY–LKA-CRNN, which limits the overall performance of the SED model. To prevent this problem, an auxiliary decoder is applied to train the SED model with BEATs embeddings. Additionally, to accommodate the different recording characteristics of sound events in the two datasets, multi-channel log-mel features are concatenated in a channel-wise manner. Finally, a maximum probability aggregation (MPA) approach is proposed to address the different labeling time intervals of the two datasets. The performance of the proposed SED model is evaluated on the validation dataset for the DCASE 2024 Challenge Task 4, in terms of class-score-based polyphonic sound detection score (PSDS) and macro-average partial area under the receiver operating characteristic curve (MpAUC). The results show that the proposed model performs better than the baseline. In addition, the proposed SED model employing the multi-channel log-mel feature, auxiliary decoder, and MPA outperforms the baseline model. Ensembling several versions of the proposed SED model improves PSDS and MpAUC, scoring 0.038 higher in the sum of PSDS and MpAUC compared to the baseline model.</div></details> |
| 32 | Large-Language-Model-Based Caption Augmentation for Language-Queried Audio Source Separation | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Song_43.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">This paper proposes a prompt-engineering-based caption augmentation approach for enhancing the performance of languagequeried audio source separation (LASS) models. In the context of LASS, when large language models (LLMs) are utilized to generate augmented captions for audio clip descriptions, the choice of LLM prompts significantly influences the performance of LASS models. Hence, this study compares the performance of a LASS model using a dataset-dependent prompt (DDP) and a dataset-independent prompt (DIP). Experimental results on a small-sized benchmarking dataset reveal that the DDP-based caption augmentation approach achieves better speech quality than the corresponding DIP approach. However, not all DDP-generated captions guarantee quality improvement of the LASS models. Thus, a criterion is proposed to exclusively select effective captions based on their Bidirectional Encoder Representations from Transformers (BERT) similarity scores relative to the original caption. Subsequently, augmented captions with BERT similarity scores exceeding a predefined threshold are adopted for model training. The effectiveness of the proposed prompt-engineering-based approach is then evaluated on the baseline LASS model of DCASE 2024 Challenge Task 9. Performance evaluation results show that the baseline LASS model using the proposed prompt-generated caption outperforms the model using the original caption. The proposed prompt-engineering approach is also applied to AudioSep, a state-of-the-art model, to verify its validity across diverse LASS models. Ablation studies reveal that selecting appropriate prompts for LLM-based caption augmentation significantly enhances LASS performance. Furthermore, selective augmentation based on BERT similarity scores can further boost audio separation quality.</div></details> |
| 33 | SALT: Standardized Audio Event Label Taxonomy | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Stamatiadis_69.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Machine listening systems often rely on fixed taxonomies to organize and label audio data, key for training and evaluating deep neural networks (DNNs) and other supervised algorithms. However, such taxonomies face significant constraints: they are composed of application-dependent predefined categories, which hinders the integration of new or varied sounds, and exhibits limited cross-dataset compatibility due to inconsistent labeling standards. To overcome these limitations, we introduce SALT: Standardized Audio event Label Taxonomy. Building upon the hierarchical structure of AudioSet’s ontology, our taxonomy extends and standardizes labels across 24 publicly available environmental sound datasets, allowing the mapping of class labels from diverse datasets to a unified system. Our proposal comes with a new Python package designed for navigating and utilizing this taxonomy, easing cross-dataset label searching and hierarchical exploration. Notably, our package allows effortless data aggregation from diverse sources, hence easy experimentation with combined datasets.</div></details> |
| 34 | Machine Listening in a Neonatal Intensive Care Unit | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Tailleur_66.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Oxygenators, alarm devices, and footsteps are some of the most common sound sources in a hospital. Detecting them has scientific value for environmental psychology but comes with challenges of its own: namely, privacy preservation and limited labeled data. In this paper, we address these two challenges via a combination of edge computing and cloud computing. For privacy preservation, we have designed an acoustic sensor which computes third-octave spectrograms on the fly instead of recording audio waveforms. For sample-efficient machine learning, we have repurposed a pretrained audio neural network (PANN) via spectral transcoding and label space adaptation. A small-scale study in a neonatological intensive care unit (NICU) confirms that the time series of detected events align with another modality of measurement: i.e., electronic badges for parents and healthcare professionals. Hence, this paper demonstrates the feasibility of polyphonic machine listening in a hospital ward while guaranteeing privacy by design.</div></details> |
| 35 | Acoustic-Based Traffic Monitoring with Neural Network Trained by Matching Loss for Ranking Function | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Takahashi_62.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">In this study, we propose an effective loss function for training neural networks (NNs) in acoustic-based traffic monitoring. This task involves estimating the number of vehicles from a fixed duration of acoustic input, such as one minute. Since the distribution of the number of passing vehicles depends on the road and can deviate significantly from a normal distribution, using Mean Square Error (MSE) as the loss function may not always lead to efficient learning. To address this, we introduce a matching loss for the ranking function into the loss function. This enhances learning by increasing the rank correlation between true and estimated vehicle counts. We evaluated the effectiveness of this loss function on the development dataset of the DCASE 2024 Challenge Task 10 under various input feature and network architecture conditions. The results demonstrate that the proposed loss function significantly improves Kendall’s Tau Rank Correlation (KTRC) and Root Mean Square Error (RMSE), highlighting its potential for improving acoustic-based traffic monitoring systems.</div></details> |
| 36 | Towards Learning a Difference-Aware General-Purpose Audio Representation | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Takeuchi_20.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">General-purpose audio representations with self-supervised learning have shown promising results on diverse tasks. Methods such as BYOL-A try to learn semantically robust representation by ignoring differences between two data computed using data augmentations that simulate semantically similar data from the same input. However, some audio-difference-related tasks require representations that are sensitive to slight semantic differences while maintaining robustness to similar data. This study investigates how to learn difference-aware audio representations. We propose subtraction-consistent representation learning in which mixed sounds are separable by subtracting representations in latent space. In the proposed method, an additional network extending BYOL-A learns the difference between a sound sample and its down-mix with another sound sample. Experiments confirmed that the proposed method improves the accuracy of difference-aware audio tasks while maintaining the general-purpose audio representation performance.</div></details> |
| 37 | The Language of Sound Search: Examining User Queries in Audio Search Engines | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Weck_54.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">This study examines textual, user-written search queries within the context of sound search engines, encompassing various applications such as foley, sound effects, and general audio retrieval. Current research inadequately addresses real-world user needs and behaviours in designing text-based audio retrieval systems. To bridge this gap, we analysed search queries from two sources: a custom survey and Freesound website query logs. The survey was designed to collect queries for an unrestricted, hypothetical sound search engine, resulting in a dataset that captures user intentions without the constraints of existing systems. This dataset is also made available for sharing with the research community. In contrast, the Freesound query logs encompass approximately 9 million search requests, providing a comprehensive view of real-world usage patterns. Our findings indicate that survey queries are generally longer than Freesound queries, suggesting users prefer detailed queries when not limited by system constraints. Both datasets predominantly feature keyword-based queries, with few survey participants using full sentences. Key factors influencing survey queries include the primary sound source, intended usage, perceived location, and the number of sound sources. These insights are crucial for developing user-centred, effective text-based audio retrieval systems, enhancing our understanding of user behaviour in sound search contexts.</div></details> |
| 38 | AdaProj: Adaptively Scaled Angular Margin Subspace Projections for Anomalous Sound Detection with Auxiliary Classification Tasks | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Wilkinghoff_1.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">The state-of-the-art approach for semi-supervised anomalous sound detection is to first learn an embedding space by using auxiliary classification tasks based on meta information or self-supervised learning and then estimate the distribution of normal data. In this work, AdaProj a novel loss function for training the embedding model is presented. In contrast to commonly used angular margin losses, which project data of each class as close as possible to their corresponding class centers, AdaProj learns to project data onto class-specific subspaces while still ensuring an angular margin between classes. By doing so, the resulting distributions of the embeddings belonging to normal data are not required to be as restrictive as other loss functions allowing a more detailed view on the data. In experiments conducted on the DCASE2022 and DCASE2023 anomalous sound detection datasets, it is shown that using AdaProj to learn an embedding space significantly outperforms other commonly used loss functions.</div></details> |
| 39 | A Reference-Free Metric for Language-Queried Audio Source Separation using Contrastive Language-Audio Pretraining | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Xiao_38.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Language-queried audio source separation (LASS) aims to separate an audio source guided by a text query, with the signal-to-distortion ratio (SDR)-based metrics being commonly used to objectively measure the quality of the separated audio. However, the SDR-based metrics require a reference signal, which is often difficult to obtain in real-world scenarios. In addition, with the SDR-based metrics, the content information of the text query is not considered effectively in LASS. This paper introduces a reference-free evaluation metric using a contrastive language-audio pretraining (CLAP) module, termed CLAPScore, which measures the semantic similarity between the separated audio and the text query. Unlike SDR, the proposed CLAPScore metric evaluates the quality of the separated audio based on the content information of the text query, without needing a reference signal. Experiments show that the CLAPScore provides an effective evaluation of the semantic relevance of the separated audio to the text query, as compared to the SDR metric, offering an alternative for the performance evaluation of LASS systems. The code for evaluation is publicly available.</div></details> |
| 40 | WildDESED: An LLM-Powered Dataset for Wild Domestic Environment Sound Event Detection System | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Xiao_2.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">This work aims to advance sound event detection (SED) research by presenting a new large language model (LLM)-powered dataset namely wild domestic environment sound event detection (WildDESED). It is crafted as an extension to the original DESED dataset to reflect diverse acoustic variability and complex noises in home settings. We leveraged LLMs to generate eight different domestic scenarios based on target sound categories of the DESED dataset. Then we enriched the scenarios with a carefully tailored mixture of noises selected from AudioSet and ensured no overlap with target sound. We consider widely popular convolutional neural recurrent network to studyWildDESED dataset, which depicts its challenging nature. We then apply curriculum learning by gradually increasing noise complexity to enhance the model’s generalization capabilities across various noise levels. Our results with this approach show improvements within the noisy environment, validating the effectiveness on the WildDESED dataset promoting noise-robust SED advancements.</div></details> |
| 41 | Integrating Continuous and Binary Relevances in Audio-Text Relevance Learning | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Xie_37.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Audio-text relevance learning refers to learning the shared semantic properties of audio samples and textual descriptions. The standard approach uses binary relevances derived from pairs of audio samples and their human-provided captions, categorizing each pair as either positive or negative. This may result in suboptimal systems due to varying levels of relevance between audio samples and captions. In contrast, a recent study used human-assigned relevance ratings, i.e., continuous relevances, for these pairs but did not obtain performance gains in audio-text relevance learning. This work introduces a relevance learning method that utilizes both humanassigned continuous relevance ratings and binary relevances using a combination of a listwise ranking objective and a contrastive learning objective. Experimental results demonstrate the effectiveness of the proposed method, showing improvements in language-based audio retrieval, a downstream task in audio-text relevance learning. In addition, we analyze how properties of the captions or audio clips contribute to the continuous audio-text relevances provided by humans or learned by the machine.</div></details> |
| 42 | Moflenet: A Low Complexity Model for Acoustic Scene Classification | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Yifei_44.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">Designing lightweight models that require minimal computational resources and can operate on edge devices is the latest trend in deep learning research. This paper details our approach to Task 1: Low-Complexity Acoustic Scene Classification (ASC) for the DCASE'24 challenge. The task involves developing data-efficient systems for five scenarios, which progressively limit the available training data (i.e., 100%, 50%, 25%, 10%, 5%), while also handling device mismatches and low-complexity constraints (maximum memory allowance for model parameters: 128 kB, maximum number of MACs per inference: 30 million). In this work, we introduce a lightweight novel CNN architecture called MofleNet, featuring a combination of shuffle channels and residual inverted bottleneck blocks. Furthermore, we improve the performance by ensembling MofleNet with CP-ResNet. To meet the constraint of keeping the model size under 128 kB, both models are fine-tuned using quantization-aware training. Compared to the DCASE'24 Task 1 baseline, our proposed system improves results on the TAU Urban Acoustic Scenes 2022 Mobile Development dataset by around 6% on an average across five datasets and 4% on the challenge test set, earning a 7th rank in the DCASE’24 task 1 challenge.</div></details> |
| 43 | Improving Language-Based Audio Retrieval using LLM Augmentations | :heavy_minus_sign: | [![DCASE](https://img.shields.io/badge/pdf-DCASE-395C5.svg)](https://dcase.community/documents/workshop2024/proceedings/DCASE2024Workshop_Zgorzynski_48.pdf) <br/>  | <details><summary id="toggle">Show</summary> <div id="content" style="display: none;">This work explores the integration of large language models (LLMs) in multimodal machine learning, focusing on their usefulness in augmenting and generating audio-caption datasets. The study is structured around three primary objectives. The first objective is to evaluate the capability of LLMs to enhance existing audio-caption datasets by generating augmented and improved captions. The second objective explores the potential of LLMs to create new audio-caption datasets by extracting relevant text and audio from video-caption datasets. Various LLMs and hyperparameter configurations are tested to determine their effectiveness in these two tasks. The final objective is to evaluate the impact of these augmented and newly created datasets on training outcomes, providing insights into their potential contributions to audio related machine learning tasks. The results demonstrate the potential of LLMs to significantly advance the field by improving data quality and availability, in result enhancing model training and performance.</div></details> |


## Challenge Tasks

| **ID** | **Category** | **Task**    |       **Baselines**             |    **Proposed Model(s)**    |                                                                                                       
|:------:|:------------:|-----------------|:---------------------------:|---------------------------|
| 1 | Scenes | Data-Efficient Low-Complexity Acoustic Scene Classification | [![GitHub](https://img.shields.io/github/stars/CPJKU/dcase2024_task1_baseline)](https://github.com/CPJKU/dcase2024_task1_baseline) | A CNN-based approach |
| 2 | Monitoring | First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring | [![GitHub](https://img.shields.io/github/stars/nttcslab/dcase2023_task2_baseline_ae)](https://github.com/nttcslab/dcase2023_task2_baseline_ae) | Autoencoder-based baseline with two operating modes [![arXiv](https://img.shields.io/badge/arXiv-2303.00455-b31b1b.svg)](https://arxiv.org/pdf/2303.00455.pdf) |
| 3 | Localization | Audio and audiovisual sound event localization and detection with source distance estimation | [![GitHub](https://img.shields.io/github/stars/partha2409/DCASE2024_seld_baseline)](https://github.com/partha2409/DCASE2024_seld_baseline) | Track A: Audio-only baseline <br/> [![arXiv](https://img.shields.io/badge/arXiv-2111.08192-b31b1b.svg)](https://arxiv.org/pdf/2111.08192.pdf) <br/> Track B: Audiovisual baseline <br/> [![arXiv](https://img.shields.io/badge/arXiv-2312.09034-b31b1b.svg)](https://arxiv.org/pdf/2312.09034.pdf) | 
| 4 | Events | Sound event detection with heterogeneous training dataset and potentially missing labels | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/DCASE-REPO/DESED_task/tree/master/recipes/dcase2024_task4_baseline) <br/> [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.11280943.svg)](https://doi.org/10.5281/zenodo.11280943) | Mean-Teacher model <br/> [![arXiv](https://img.shields.io/badge/arXiv-2406.08056-b31b1b.svg)](https://arxiv.org/abs/2406.08056) |
| 5 | Bio | Few-shot Bioacoustic Event Detection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/c4dm/dcase-few-shot-bioacoustic/tree/main/baselines/dcase2024_task5) | Prototypical Network with negative sampling <br/> [![arXiv](https://img.shields.io/badge/arXiv-2403.18638-b31b1b.svg)](https://arxiv.org/pdf/2403.18638.pdf) |
| 6 | Caption | Automated Audio Captioning | [![GitHub](https://img.shields.io/github/stars/Labbeti/dcase2024-task6-baseline)](https://github.com/Labbeti/dcase2024-task6-baseline) | A sequence-to-sequence system | 
| 7 | Synthesis | Sound Scene Synthesis | [![GitHub](https://img.shields.io/github/stars/DCASE2024-Task7-Sound-Scene-Synthesis/AudioLDM-training-finetuning)](https://github.com/DCASE2024-Task7-Sound-Scene-Synthesis/AudioLDM-training-finetuning) | AudioLDM <br/> [![ICML](https://img.shields.io/badge/pdf-ICML-218.svg)](https://proceedings.mlr.press/v202/liu23f/liu23f.pdf) |
| 8 | Retrieval | Language-Based Audio Retrieval | [![GitHub](https://img.shields.io/github/stars/xieh97/dcase2023-audio-retrieval)](https://github.com/xieh97/dcase2023-audio-retrieval) | A bi-encoder architecture with a pre-trained CNN14 (see PANNs [![arXiv](https://img.shields.io/badge/arXiv-1912.10211-b31b1b.svg)](https://arxiv.org/pdf/1912.10211.pdf)) being the audio encoder and the Sentence-BERT (i.e., "all-mpnet-base-v2") being the text encoder [![arXiv](https://img.shields.io/badge/arXiv-1908.10084-b31b1b.svg)](https://arxiv.org/pdf/1908.10084.pdf) | 
| 9 | Separation | Language-Queried Audio Source Separation | [![GitHub](https://img.shields.io/github/stars/Audio-AGI/dcase2024_task9_baseline)](https://github.com/Audio-AGI/dcase2024_task9_baseline) | LASS-Net <br/> [![arXiv](https://img.shields.io/badge/arXiv-2203.15147-b31b1b.svg)](https://arxiv.org/pdf/2203.15147.pdf) |
| 10| Monitoring | Acoustic-based traffic monitoring | [![GitHub](https://img.shields.io/github/stars/boschresearch/acoustic-traffic-simulation-counting)](https://github.com/boschresearch/acoustic-traffic-simulation-counting) | A Convolutional Recurrent Neural Network (CRNN) based architecture <br/> [![DAFx20in22](https://img.shields.io/badge/pdf-DAFx20in22-2188.svg)](https://www.dafx.de/paper-archive/2022/papers/DAFx20in22_paper_9.pdf) <br/> [![arXiv](https://img.shields.io/badge/arXiv-2401.09308-b31b1b.svg)](https://arxiv.org/pdf/2401.09308.pdf)

## Datasets

| **ID** | **Title**                                                  | **Tasks** | **Year** |                                                                                                                                  **Repo**                                                                                                                                   |
|:------:|------------------------------------------------------------|:---------:|:--------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
|   1    | TAU Urban Acoustic Scenes 2022 Mobile, Development dataset |     1     |   2022   |              1. TAU Urban Acoustic Scenes [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6337421.svg)](https://doi.org/10.5281/zenodo.6337421) <br/> 2. [Development-train splits](https://github.com/CPJKU/dcase2024_task1_baseline/releases/tag/files)               |
|   2    | Task 2, Development dataset                                |     2     | 2024 |                                                                                   [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10902294.svg)](https://zenodo.org/records/10902294)                                                                                   |
|   3    | Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23)     |     3     | 2023 |                                                                                    [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7709052.svg)](https://zenodo.org/records/7709052)                                                                                    |
|   4    | DESED dataset + MAESTRO dataset                            |     4     | 2024 | 1. [DESED data generation script](https://github.com/DCASE-REPO/DESED_task/blob/master/recipes/dcase2021_task4_baseline/generate_dcase_task4_2021.py) <br/> 2. MAESTRO [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7244360.svg)](https://zenodo.org/record/7244360) |
|   5    | Task 5, Development dataset |     5     | 2024 |                                                                                 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10829604.svg)](https://doi.org/10.5281/zenodo.10829604)                                                                                 |
|   6    | Clotho v2.1 |     6     | 2024 |                                                                                    [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3490683.svg)](https://zenodo.org/record/3490683)                                                                                     |
|   7    | Task 7, Development dataset |     7     | 2024 |                                                                                   [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10869644.svg)](https://zenodo.org/records/10869644)                                                                                   |
| 8 | Clotho v2.1 |     8     | 2024 |                                                                                    [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4783391.svg)](https://zenodo.org/record/4783391)                                                                                     |
| 9 | DCASE 2024 LASS Development + Validation(synth) | 9 | 2024 | 1. Development [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10887496.svg)](https://zenodo.org/records/10887496) <br/> 2. Validation [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10886481.svg)](https://zenodo.org/records/10886481)             |
| 10 | Task 10, Development dataset | 10 | 2024 |                                                                                   [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10700792.svg)](https://zenodo.org/records/10700792)                                                                                   |


## Key Terms
<img src="dcase2024-wordcloud-u.png" width="624"/>
